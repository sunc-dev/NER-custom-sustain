{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font size=\"10\">Custom entity recognition </font>\n",
    "## Training data environment setup\n",
    "\n",
    "## Introduction:\n",
    "\n",
    "This notebook is test code to setup new entities and programmtically creating training data to train the NER model. The goal of the project is to tag environmental/sustainability technologies contained with in the systems data stored with in our capital planning systems.\n",
    "\n",
    "To skip to creation of training data and go directly to [model training and model implementation](./ner-model-note.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP\n",
    "Module downloads (necessary for only first instance): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!pip install pdfminer.six\n",
    "#!python -m spacy download en_core_web_lg\n",
    "#general imports \n",
    "#!pip install nltk \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from io import StringIO\n",
    "\n",
    "#pdf miner \n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "#SpaCy\n",
    "import en_core_web_sm\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.pipeline import Sentencizer\n",
    "from spacy.lemmatizer import Lemmatizer, ADJ, NOUN, VERB\n",
    "\n",
    "#nltk\n",
    "from nltk import tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing data setup\n",
    "The code section below setup the testing for the NER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative path setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filepath is : ./test/systems.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "root = r'./test/'\n",
    "file = 'systems.csv'\n",
    "print('Filepath is :',(os.path.join( root, file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaration and store testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append comments together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = [_ for _ in data['System - Comments']]\n",
    "description = [_ for _ in data['System - Description']]\n",
    "\n",
    "#combine list of text data\n",
    "for i in description:\n",
    "  comments.append(i)\n",
    "  \n",
    "#clean test\n",
    "comments = [x for x in comments if str(x) != 'nan']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data setup\n",
    "The code section below setup the training data for the NER.\n",
    "The is defined in the following steps:\n",
    "* Convert documents into text\n",
    "* Setting up the SpaCy nlp object\n",
    "* Load, parsing and tokenization of text into a list of sentences\n",
    "* Utility function order to parse the tokenized sentences in order to inherit the form:  [(Sentence, {entities: [(start, end, label)]}, ...]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy object delcaration and relative path setup:\n",
    "\n",
    "SpaCy requires training data to be in the format: [(Sentence, {entities: [(start, end, label)]}, ...].\n",
    "We first need to create a list of sentences throughout a doc object containing the new entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Language.pipe of <spacy.lang.en.English object at 0x000001DEA6F70880>>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Declare english vocab for our nlp object to define english language sentence boundaries\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp = English()\n",
    "nlp.pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare rules for sentence boundary detection logic and add to the nlp pipe object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentencizer = Sentencizer(punct_chars=[\".\", \"?\", \"!\", \"。\"])\n",
    "nlp.add_pipe(sentencizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup import doc import, write tests for single doc object for now:\n",
    "\n",
    "## Current test case is for the word 'green roof'\n",
    "\n",
    "Document path setup (can change to a higher level directory to capture more/different docs for other terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WindowsPath('doc/greenr/example1.pdf'), WindowsPath('doc/greenr/example2.pdf'), WindowsPath('doc/greenr/example3.PDF'), WindowsPath('doc/greenr/example4.pdf')]\n"
     ]
    }
   ],
   "source": [
    "doc_path = Path(r'./doc/greenr/') #change this for broader directory to capture\n",
    "pdf_files = list(doc_path.glob('*.pdf'))  # convert result to a list\n",
    "print(pdf_files)\n",
    "\n",
    "#test on single pdf object\n",
    "#pdf_object =  os.path.join(doc_path, 'example2.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert pdf object into text format to parse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "texts = []\n",
    "for file in pdf_files:\n",
    "    with open(file,'rb') as pdf:\n",
    "        text = extract_text(pdf)\n",
    "        texts.append(text)\n",
    "#check object\n",
    "#texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence tokenization using SpaCy Sentencizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines : 2830\n"
     ]
    }
   ],
   "source": [
    "# alternative code for sentence tokenization using nltk\n",
    "# tokens = tokenize.sent_tokenize(text)\n",
    "\n",
    "#sentence tokenization using english vocab and SpaCy Sentencizer\n",
    "#declare doc object to pass into our nlp pipeline\n",
    "all_text = \" \".join(str(x) for x in texts)    \n",
    "doc = nlp(all_text)\n",
    "\n",
    "#check number of sentences detected by Sentencizer\n",
    "print(\"Number of lines :\", (len(list(doc.sents))))\n",
    "\n",
    "# create a list of tokenized sentences\n",
    "sents = [sent.text.strip() for sent in doc.sents]\n",
    "#check sents list object\n",
    "#sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECLARE NEW CUSTOM ENTITY\n",
    "Code below setups and defines new custom entity and prepares the training data for the NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New entity creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ruled based matching using PhraseMatcher - edit to add additional token patterns to capture more strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entity creation\n",
    "#Label setup, we provide the label 'SUSTECH' for sustainability and resilience technologies.\n",
    "LABEL = 'SUSTECH'\n",
    "\n",
    "'''Define matcher object using english vocab we defined earlier. Test using PhraseMatcher\n",
    "to define rules based on the exact patterns the strings will take the form of'''\n",
    "\n",
    "#Character patterns to add into our matcher object\n",
    "rule_matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# create rule patterns - ADD more rule patterns here!\n",
    "rule_patterns = ['Green roof', \n",
    "            'green roof', \n",
    "            'green Roof',\n",
    "            'Green Roof']\n",
    "\n",
    "for i in rule_patterns: \n",
    "    rule_matcher.add(LABEL, None, nlp(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for pattern detection defined using phraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a broken green roof in my Green Roof hood.\n",
      "green roof\n",
      "Green Roof\n"
     ]
    }
   ],
   "source": [
    "# A simple test to check our matcher object\n",
    "test_doc = nlp(\"I have a broken green roof in my Green Roof hood.\")\n",
    "print(test_doc)\n",
    "\n",
    "for idx, start, end in rule_matcher(test_doc):\n",
    "    print(test_doc[start:end],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token based matching using Matcher - edit to add additional token patterns to capture more strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using Matcher\n",
    "\n",
    "'''Define matcher object using english vocab we defined earlier. \n",
    "Test using Matcher to define rules based on the token attributes of the string.'''\n",
    "token_matcher = Matcher(nlp.vocab, validate=True)\n",
    "\n",
    "# create token patterns - ADD more token patterns here!\n",
    "token_patterns = [[{\"LOWER\": \"green\"}, \n",
    "                   {\"LOWER\": \"roof\"}, \n",
    "                  ],\n",
    "                 ]\n",
    "\n",
    "\n",
    "token_matcher.add(LABEL, None, *token_patterns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a broken green roof in my Green Roof hood.\n",
      "green roof\n",
      "Green Roof\n"
     ]
    }
   ],
   "source": [
    "test_doc = nlp(\"I have a broken green roof in my Green Roof hood.\")\n",
    "print(test_doc)\n",
    "\n",
    "for idx, start, end in token_matcher(test_doc):\n",
    "    print(test_doc[start:end],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility function\n",
    "Recall in order to train the NER model, we require to annotate tokenized text that takes the form: [(Sentence, {entities: [(start, end, label)]}, ...]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define our utility function\n",
    "def train_parser(doc):\n",
    "    position = [(doc[start:end].start_char, doc[start:end].end_char, LABEL) for \n",
    "                  idx, start, end in token_matcher(doc)]\n",
    "    return (doc.text, {'entities': position})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests for our utility function on our test doc object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I have a broken green roof in my Green Roof hood.',\n",
       " {'entities': [(16, 26, 'SUSTECH'), (33, 43, 'SUSTECH')]})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_parser(test_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying our utility function to our doc object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('From a hydrologic \\nperspective, the green roof acts like a lawn or meadow by storing rainwater in the \\ngrowing medium and ponding areas.',\n",
       "  {'entities': [(36, 46, 'SUSTECH')]}),\n",
       " ('Guidance in this guide \\nfocuses on extensive green roof design.',\n",
       "  {'entities': [(45, 55, 'SUSTECH')]}),\n",
       " ('Some \\nmunicipalities, such as the City of Toronto, offer green roof incentive programs \\nthat should be considered in the cost assessment.',\n",
       "  {'entities': [(57, 67, 'SUSTECH')]}),\n",
       " ('A study of the life cycle costs \\nand savings of building and owning a green roof in the Greater Toronto Area was \\nundertaken by TRCA (2007a).',\n",
       "  {'entities': [(70, 80, 'SUSTECH')]}),\n",
       " ('4-24 \\n\\nVersion 1.0 \\n\\n\\x0c \\n\\n \\n\\nLow Impact Development Stormwater Management Planning and Design Guide \\n\\nFigure 4.2.2  A green roof during winter \\n\\n \\n\\nSource: National Research Council Canada, 2006 \\n\\n \\nPhysical Suitability and Constraints \\nGreen roofs are physically feasible in most development situations, but should be \\nplanned at the time of building design.',\n",
       "  {'entities': [(117, 127, 'SUSTECH')]}),\n",
       " ('Table 4.2.2  Monitoring results – green roof runoff reduction  \\n\\nLocation \\n\\nMonitoring Period \\n\\nToronto, Ontario \\n\\nToronto, Ontario \\n\\nOttawa, Ontario \\nEast Lansing, \\nMichigan \\nEast Lansing, \\nMichigan \\n\\nMay ’03 – Aug.',\n",
       "  {'entities': [(34, 44, 'SUSTECH')]}),\n",
       " ('Values represent total precipitation retained by the green roof over the monitoring period unless \\n\\n2.',\n",
       "  {'entities': [(53, 63, 'SUSTECH')]}),\n",
       " ('Value represents reduction in runoff from the green roof relative to a reference roof, not relative to \\n\\n3.',\n",
       "  {'entities': [(46, 56, 'SUSTECH')]}),\n",
       " ('A TRCA study comparing conventional black roof runoff to green roof \\nrunoff in Toronto was completed in 2006.',\n",
       "  {'entities': [(57, 67, 'SUSTECH')]}),\n",
       " ('The loading ‘percent difference’ values shown in the right column represent the \\ndifference in loading, expressed as a percentage, between unit area loads from the \\nconventional roof and the green roof.',\n",
       "  {'entities': [(191, 201, 'SUSTECH')]})]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA = [train_parser(d) for d in nlp.pipe(sents) if len(token_matcher(d))==1]\n",
    "TRAIN_DATA[0:10]\n",
    "#len(TRAIN_DATA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = r'./train/'\n",
    "TRAIN_DATA_OUTPUT = pd.DataFrame(TRAIN_DATA, columns=['text',\n",
    "                                                      'position'])\n",
    "TRAIN_DATA_OUTPUT.to_csv(os.path.join(train_path,'train.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
