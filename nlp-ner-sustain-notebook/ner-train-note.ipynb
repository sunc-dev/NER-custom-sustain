{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font size=\"10\">Custom entity recognition </font>\n",
    "## Training environment setup\n",
    "\n",
    "This notebook is test code to setup new entities and training data for the NER model. The goal of the project is to tag environmental/sustainability technologies contained with in the systems data stored with in our capital planning systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module downloads (necessary for only first instance): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!pip install pdfminer.six\n",
    "#!python -m spacy download en_core_web_lg\n",
    "#general imports \n",
    "#!pip install nltk \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#pdf miner \n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "\n",
    "#SpaCy\n",
    "import en_core_web_sm\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.pipeline import Sentencizer\n",
    "from spacy.lemmatizer import Lemmatizer, ADJ, NOUN, VERB\n",
    "\n",
    "#nltk\n",
    "from nltk import tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing data setup\n",
    "The code section below setup the testing for the NER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative path setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filepath is : ./data/systems.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "root = r'./test/'\n",
    "file = 'systems.csv'\n",
    "print('Filepath is :',(os.path.join( root, file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaration and store testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append comments together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = [_ for _ in data['System - Comments']]\n",
    "description = [_ for _ in data['System - Description']]\n",
    "\n",
    "#combine list of text data\n",
    "for i in description:\n",
    "  comments.append(i)\n",
    "  \n",
    "#clean test\n",
    "comments = [x for x in comments if str(x) != 'nan']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data setup\n",
    "The code section below setup the training data for the NER.\n",
    "The is defined in the following steps:\n",
    "* Convert documents into text\n",
    "* Setting up the SpaCy nlp object\n",
    "* Load, parsing and tokenization of text into a list of sentences\n",
    "* Utility function order to parse the tokenized sentences in order to inherit the form:  [(Sentence, {entities: [(start, end, label)]}, ...]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy object delcaration and relative path setup:\n",
    "\n",
    "SpaCy requires training data to be in the format: [(Sentence, {entities: [(start, end, label)]}, ...].\n",
    "We first need to create a list of sentences throughout a doc object containing the new entity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Language.pipe of <spacy.lang.en.English object at 0x000002A76FE08580>>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Declare english vocab for our nlp object to define english language sentence boundaries\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp = English()\n",
    "nlp.pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare rules for sentence boundary detection logic and add to the nlp pipe object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentencizer = Sentencizer(punct_chars=[\".\", \"?\", \"!\", \"ã€‚\"])\n",
    "nlp.add_pipe(sentencizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup import doc import, write tests for single doc object for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_path = r'./doc/greenr/'\n",
    "pdf_object =  os.path.join(doc_path, 'example2.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert pdf object into text format to parse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "with open(pdf_object,'rb') as pdf:\n",
    "    text = extract_text(pdf)\n",
    "    \n",
    "#check object\n",
    "#text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence tokenization using SpaCy Sentencizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative code for sentence tokenization using nltk\n",
    "# tokens = tokenize.sent_tokenize(text)\n",
    "\n",
    "#sentence tokenization using english vocab and SpaCy Sentencizer\n",
    "#declare doc object to pass into our nlp pipeline\n",
    "doc = nlp(text)\n",
    "\n",
    "#check number of sentences detected by Sentencizer\n",
    "len(list(doc.sents))\n",
    "\n",
    "# create a list of tokenized sentences\n",
    "sents = [sent.text for sent in doc.sents]\n",
    "#check sents list object\n",
    "# sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare new custom entity\n",
    "Code below setups and defines new custom entity and prepares the training data for the NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New entity creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern detection using PhraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entity creation\n",
    "#Label setup, we provide the label 'SUSTECH' for sustainability and resilience technologies.\n",
    "label = 'SUSTECH'\n",
    "\n",
    "'''Define matcher object using english vocab we defined earlier. Test using PhraseMatcher\n",
    "to define rules based on the exact patterns the strings will take the form of'''\n",
    "\n",
    "#Character patterns to add into our matcher object\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "patterns = ['Green roof', \n",
    "            'green roof', \n",
    "            'green Roof',\n",
    "            'Green Roof']\n",
    "\n",
    "for i in patterns: \n",
    "    matcher.add(label, None, nlp(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for pattern detection defined using phraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a fucking green roof in my Green Roof hood.\n",
      "green roof\n",
      "Green Roof\n"
     ]
    }
   ],
   "source": [
    "# A simple test to check our matcher object\n",
    "doc = nlp(\"I have a broken green roof in my Green Roof hood.\")\n",
    "print(doc)\n",
    "\n",
    "for idx, start, end in matcher(doc):\n",
    "    print(doc[start:end],)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern detection using Matcher - (to be defined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using Matcher\n",
    "\n",
    "'''Define matcher object using english vocab we defined earlier. \n",
    "Test using Matcher to define rules based on the token attributes of the string.'''\n",
    "matcher = Matcher(nlp.vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility function\n",
    "Recall in order to train the NER model, we require to annotate tokenized text that takes the form: [(Sentence, {entities: [(start, end, label)]}, ...]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define our utility function\n",
    "def train_parser(doc):\n",
    "    detections = [(doc[start:end].start_char, doc[start:end].end_char, label) for \n",
    "                  idx, start, end in matcher(doc)]\n",
    "    return (doc.text, {'entities': detections})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests for our utility function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I have a fucking green roof in my Green Roof hood.',\n",
       " {'entities': [(17, 27, 'SUSTECH'), (34, 44, 'SUSTECH')]})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_parser(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying our utility function to our doc object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('As green infrastructure, green roof benefits \\nare  plenty.',\n",
       "  {'entities': [(25, 35, 'SUSTECH')]}),\n",
       " (' \\n \\nSection 4 explores the need for a green roof policy for Malta.',\n",
       "  {'entities': [(38, 48, 'SUSTECH')]}),\n",
       " ('  \\n \\nSection 9, looks at local planning and construction policies to identify whether they support \\ngreen roof technology.',\n",
       "  {'entities': [(100, 110, 'SUSTECH')]})]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA = [train_parser(d) for d in nlp.pipe(sents) if len(matcher(d))==1]\n",
    "TRAIN_DATA[5:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = r'./train/'\n",
    "TRAIN_DATA_OUTPUT = pd.DataFrame(TRAIN_DATA)\n",
    "TRAIN_DATA_OUTPUT.to_csv(os.path.join(train_path,'train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
