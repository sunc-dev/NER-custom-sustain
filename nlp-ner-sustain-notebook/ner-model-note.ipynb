{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font size=\"10\">Custom entity recognition </font>\n",
    "## Model environment setup\n",
    "\n",
    "This notebook contains test code to train the implemented model in the generated training data outputted from\n",
    "the [ner-train notebook located here](./ner-train-note.ipynb).\n",
    "\n",
    "For simple loop model training (old), go [here](## Run training - using simple training loop from blank -- Old)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import warnings\n",
    "import json\n",
    "import ast\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "##SpaCy\n",
    "\n",
    "import en_core_web_sm\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.pipeline import Sentencizer\n",
    "from spacy.lemmatizer import Lemmatizer, ADJ, NOUN, VERB\n",
    "from spacy.util import minibatch, compounding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import training data\n",
    "\n",
    "lets import the training data we generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filepath is : ./train/train.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "train_path = r'./train/train.csv'\n",
    "print('Filepath is :',(os.path.join( train_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the advantages of including a green r...</td>\n",
       "      <td>{'entities': [(39, 49, 'SUSTECH')]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lu, J.; Yuan, J.; Yang, J.; Yang, Z. Responses...</td>\n",
       "      <td>{'entities': [(122, 132, 'SUSTECH')]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In total, 135 residences and businesses applie...</td>\n",
       "      <td>{'entities': [(56, 66, 'SUSTECH')]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The substrates consist mainly of mineral mater...</td>\n",
       "      <td>{'entities': [(126, 136, 'SUSTECH')]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8\\n\\n\f",
       "F\\nO\\nO\\nR\\n\\n \\n\\nN\\nE\\nE\\nR\\nG\\n\\n \\n\\...</td>\n",
       "      <td>{'entities': [(150, 160, 'SUSTECH')]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[CrossRef]\\nSpeak, A.F.; Rothwell, J.J.; Lindl...</td>\n",
       "      <td>{'entities': [(124, 134, 'SUSTECH')]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>In addition, it should be considered that the ...</td>\n",
       "      <td>{'entities': [(135, 145, 'SUSTECH')]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Until then, there are many niche opportunities...</td>\n",
       "      <td>{'entities': [(60, 70, 'SUSTECH')]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>If a green roof is part of the initial design ...</td>\n",
       "      <td>{'entities': [(5, 15, 'SUSTECH')]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Some of the \\nnew membranes developed specific...</td>\n",
       "      <td>{'entities': [(54, 64, 'SUSTECH')]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  What are the advantages of including a green r...   \n",
       "1  Lu, J.; Yuan, J.; Yang, J.; Yang, Z. Responses...   \n",
       "2  In total, 135 residences and businesses applie...   \n",
       "3  The substrates consist mainly of mineral mater...   \n",
       "4  8\\n\\n\n",
       "F\\nO\\nO\\nR\\n\\n \\n\\nN\\nE\\nE\\nR\\nG\\n\\n \\n\\...   \n",
       "5  [CrossRef]\\nSpeak, A.F.; Rothwell, J.J.; Lindl...   \n",
       "6  In addition, it should be considered that the ...   \n",
       "7  Until then, there are many niche opportunities...   \n",
       "8  If a green roof is part of the initial design ...   \n",
       "9  Some of the \\nnew membranes developed specific...   \n",
       "\n",
       "                                position  \n",
       "0    {'entities': [(39, 49, 'SUSTECH')]}  \n",
       "1  {'entities': [(122, 132, 'SUSTECH')]}  \n",
       "2    {'entities': [(56, 66, 'SUSTECH')]}  \n",
       "3  {'entities': [(126, 136, 'SUSTECH')]}  \n",
       "4  {'entities': [(150, 160, 'SUSTECH')]}  \n",
       "5  {'entities': [(124, 134, 'SUSTECH')]}  \n",
       "6  {'entities': [(135, 145, 'SUSTECH')]}  \n",
       "7    {'entities': [(60, 70, 'SUSTECH')]}  \n",
       "8     {'entities': [(5, 15, 'SUSTECH')]}  \n",
       "9    {'entities': [(54, 64, 'SUSTECH')]}  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA = pd.read_csv(os.path.join(train_path))\n",
    "DATA[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['What are the advantages of including a green roof in the design phase of a project,\\n\\nrather than in a retrofit situation?', {'entities': [(39, 49, 'SUSTECH')]}], ['Lu, J.; Yuan, J.; Yang, J.; Yang, Z. Responses of morphology and drought tolerance of Sedum lineare to\\nwatering regime in green roof system: A root perspective.', {'entities': [(122, 132, 'SUSTECH')]}]]\n"
     ]
    }
   ],
   "source": [
    "#convert to list for model intake\n",
    "TRAIN_DATA = DATA.values.tolist()\n",
    "\n",
    "#for element in index 1 convert string (Entity position) to dictionary to be able to read by the model function\n",
    "for position in TRAIN_DATA:\n",
    "    position[1]=ast.literal_eval(position[1])\n",
    "    \n",
    "#Check our input list\n",
    "print(TRAIN_DATA[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a test before training\n",
    "### Test existing default spacy model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x164ab1da340>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x164aaf12100>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x164aaf12b80>)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\csunj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\spacy\\displacy\\__init__.py:189: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
      "  warnings.warn(Warnings.W006)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Here is a green roof on this house. A green roof is good.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp('Here is a green roof on this house. A green roof is good.')\n",
    "displacy.render(doc, style=\"ent\")\n",
    "# verified green roof does not match an entity in the NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING THE MODEL\n",
    "## Train model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# setup our new label\\nLABEL = \\'SUSTECH\\'\\n\\n# create blank nlp model\\ndef blank_nlp_model(train_data):\\n    nlp = spacy.blank(\"en\")\\n    ner = nlp.create_pipe(\"ner\")\\n    nlp.add_pipe(ner, last=True)\\n    ner = nlp.get_pipe(\"ner\")\\n    for _, annotations in train_data:\\n        for ent in annotations.get(\"entities\"):\\n            ner.add_label(ent[2])\\n    return nlp'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# setup our new label\n",
    "LABEL = 'SUSTECH'\n",
    "\n",
    "# create blank nlp model\n",
    "def blank_nlp_model(train_data):\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    ner = nlp.create_pipe(\"ner\")\n",
    "    nlp.add_pipe(ner, last=True)\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "    for _, annotations in train_data:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "    return nlp'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training - using simple training loop from blank -- Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if model is None:\\n    nlp = blank_nlp_model(TRAIN_DATA)\\n    optimizer = nlp.begin_training()\\nfor i in range(20):\\n    losses = {}\\n    batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\\n    for batch in batches:\\n        texts, annotations = zip(*batch)\\n        nlp.update(\\n            texts,  # batch of texts\\n            annotations,  # batch of annotations\\n            drop=0.1,  # dropout - make it harder to memorise data\\n            losses=losses,\\n        )\\n    print(f\"Losses at iteration {i} - {dt.datetime.now()} {losses}\")'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''if model is None:\n",
    "    nlp = blank_nlp_model(TRAIN_DATA)\n",
    "    optimizer = nlp.begin_training()\n",
    "for i in range(20):\n",
    "    losses = {}\n",
    "    batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(\n",
    "            texts,  # batch of texts\n",
    "            annotations,  # batch of annotations\n",
    "            drop=0.1,  # dropout - make it harder to memorise data\n",
    "            losses=losses,\n",
    "        )\n",
    "    print(f\"Losses at iteration {i} - {dt.datetime.now()} {losses}\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training - performance enhancements -- Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nlp = blank_nlp_model(TRAIN_DATA)'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''nlp = blank_nlp_model(TRAIN_DATA)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x1dfbe73ff10>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x1dfbe39b400>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x1dfbe39b1c0>)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''nlp.pipeline'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'optimizer = nlp.begin_training()\\nfor i in range(20):\\n    losses = {}\\n    batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\\n    for batch in batches:\\n        texts, annotations = zip(*batch)\\n        nlp.update(\\n            texts,  # batch of texts\\n            annotations,  # batch of annotations\\n            drop=0.1,  # dropout - make it harder to memorise data\\n            losses=losses,\\n        )\\n    print(f\"Losses at iteration {i} - {dt.datetime.now()} {losses}\")'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''optimizer = nlp.begin_training()\n",
    "for i in range(20):\n",
    "    losses = {}\n",
    "    batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(\n",
    "            texts,  # batch of texts\n",
    "            annotations,  # batch of annotations\n",
    "            drop=0.1,  # dropout - make it harder to memorise data\n",
    "            losses=losses,\n",
    "        )\n",
    "    print(f\"Losses at iteration {i} - {dt.datetime.now()} {losses}\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New training model loop to either accept existing model, if not model is not defined then create a blank nlp model using english vocab\n",
    "\n",
    "def train_model(**model_params):\n",
    "    \n",
    "    model = model_params['model']\n",
    "    iterations = model_params['iterations']\n",
    "    train_data = model_params['train_data']\n",
    "    \n",
    "    \n",
    "    random.seed(0)\n",
    "    \n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model) #load existing spacy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")\n",
    "        print(\"Created blank 'en' model\")\n",
    "    \n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise,get it, so we can add labels to it\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    #ner.add_label(LABEL)  # add new entity label to entity recognizer\n",
    "    for _, annotations in train_data:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "            #print(ent[2])\n",
    "\n",
    "    # Adding extraneous labels shouldn't mess anything up\n",
    "    ner.add_label(\"VEGETABLE\")\n",
    "    move_names = list(ner.move_names)\n",
    "    # get names of other pipes to disable them during training\n",
    "    \n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    \n",
    "    # only train NER\n",
    "    \n",
    "    with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
    "        # show warnings for misaligned entity spans once\n",
    "        \n",
    "        warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "\n",
    "        sizes=compounding(4.0, 32.0, 1.001)\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "                  \n",
    "        if model is None:\n",
    "            optimizer = nlp.begin_training()\n",
    "        else:\n",
    "            optimizer = nlp.resume_training()\n",
    "\n",
    "        # reset and initialize the weights randomly â€“ but only if we're\n",
    "        # training a new model\n",
    "        for itn in range(iterations):\n",
    "            random.shuffle(train_data)\n",
    "            batches = minibatch(train_data, size=sizes)\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, \n",
    "                           annotations,\n",
    "                           sgd=optimizer, drop=0.1, \n",
    "                           losses=losses)\n",
    "            print(f\"Losses at iteration {itn} - {dt.datetime.now()} {losses}\")\n",
    "    \n",
    "    print('Model training completed')\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'model': None,\n",
    "    'iterations': 20,\n",
    "    'train_data': TRAIN_DATA\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\csunj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\spacy\\language.py:635: UserWarning: [W033] Training a new parser or NER using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed. The languages with lexeme normalization tables are currently: da, de, el, en, id, lb, pt, ru, sr, ta, th.\n",
      "  proc.begin_training(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses at iteration 0 - 2020-10-20 15:06:50.747508 {'ner': 1002.4988477857921}\n",
      "Losses at iteration 1 - 2020-10-20 15:06:56.844070 {'ner': 0.3382388563186799}\n",
      "Losses at iteration 2 - 2020-10-20 15:07:03.009030 {'ner': 2.314843076717813e-06}\n",
      "Losses at iteration 3 - 2020-10-20 15:07:08.145071 {'ner': 1.4389402114935229e-06}\n",
      "Losses at iteration 4 - 2020-10-20 15:07:13.444033 {'ner': 6.970059375317793e-08}\n",
      "Losses at iteration 5 - 2020-10-20 15:07:18.763029 {'ner': 4.415401743625468e-09}\n",
      "Losses at iteration 6 - 2020-10-20 15:07:23.210071 {'ner': 1.0562531943584313e-09}\n",
      "Losses at iteration 7 - 2020-10-20 15:07:28.005063 {'ner': 2.4602129852783327e-08}\n",
      "Losses at iteration 8 - 2020-10-20 15:07:32.702033 {'ner': 1.4264602461743666e-08}\n",
      "Losses at iteration 9 - 2020-10-20 15:07:36.751033 {'ner': 1.4091154084281965e-08}\n",
      "Losses at iteration 10 - 2020-10-20 15:07:40.465065 {'ner': 2.3211939862835905e-09}\n",
      "Losses at iteration 11 - 2020-10-20 15:07:44.450028 {'ner': 2.7417796637739753e-09}\n",
      "Losses at iteration 12 - 2020-10-20 15:07:49.055040 {'ner': 2.2936078294338676e-09}\n",
      "Losses at iteration 13 - 2020-10-20 15:07:53.011035 {'ner': 2.744960024287457e-09}\n",
      "Losses at iteration 14 - 2020-10-20 15:07:57.238033 {'ner': 6.697120240668193e-09}\n",
      "Losses at iteration 15 - 2020-10-20 15:08:00.874069 {'ner': 2.7782743667680312e-09}\n",
      "Losses at iteration 16 - 2020-10-20 15:08:04.450069 {'ner': 4.177030853905705e-08}\n",
      "Losses at iteration 17 - 2020-10-20 15:08:08.038031 {'ner': 1.968123540932064e-09}\n",
      "Losses at iteration 18 - 2020-10-20 15:08:11.961067 {'ner': 3.341604015179471e-10}\n",
      "Losses at iteration 19 - 2020-10-20 15:08:15.925056 {'ner': 5.766113064551721e-10}\n",
      "Model training completed\n"
     ]
    }
   ],
   "source": [
    "nlp = train_model(**model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x164aada3c40>)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('Here is a green roof on this house. A green roof is good.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Here is a \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    green roof\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SUSTECH</span>\n",
       "</mark>\n",
       " on this house. A \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    green roof\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SUSTECH</span>\n",
       "</mark>\n",
       " is good.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to model\n"
     ]
    }
   ],
   "source": [
    "output_dir = r'./model'\n",
    "\n",
    "if output_dir is not None:\n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "    nlp.to_disk(output_dir)\n",
    "    print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and testing the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from ./model\n",
      "Entities [('green roof', 'SUSTECH')]\n",
      "Tokens [('i', '', 2), ('am', '', 2), ('a', '', 2), ('green', 'SUSTECH', 3), ('roof', 'SUSTECH', 1)]\n"
     ]
    }
   ],
   "source": [
    "output_dir = r'./model'\n",
    "x = ['i am a green roof']\n",
    "print(\"Loading from\", output_dir)\n",
    "nlp2 = spacy.load(output_dir)\n",
    "for text in x:\n",
    "    doc = nlp2(text)\n",
    "    print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "    print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
